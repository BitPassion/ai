---
title: OpenAI
---

import { Steps, Callout } from 'nextra-theme-docs'

# OpenAI Functions

The Vercel AI SDK has **experimental client-side only support** for [OpenAI functions](https://openai.com/blog/function-calling-and-other-api-updates).
Any of the content below is subject to change as OpenAI continues to develop their functions API and we iterate on the Vercel AI SDK.
Server-side support is a work-in-progress and you can see the Vercel teams outline [here](https://twitter.com/jaredpalmer/status/1673350963191508993).
If you are unfamiliar with OpenAI functions, it's recommended you refer to OpenAI's [announcement post](https://openai.com/blog/function-calling-and-other-api-updates) instead.
It's important to know that OpenAI does _not_ handle calling the functions, but instead passes the function call to the client to handle via a special message and JSON.

## Usage with client hooks

### Function Handler

You can currently handle function calls using the `useCompletion` and `useChat` hooks. In short, you create a Function Handler and pass it to the hook.
The handler will be called when the function is invoked. Here's a sample function handler:

```tsx
const functionCallHandler: FunctionCallHandler = async (
  chatMessages,
  functionCall
) => {
  if (functionCall.name === 'get_current_weather') {
    if (functionCall.arguments) {
      const parsedFunctionCallArguments = JSON.parse(functionCall.arguments)
      // You now have access to the parsed arguments here (assuming the JSON was valid)
      // If JSON is invalid, return an appropriate message to the model so that it may retry?
      console.log(parsedFunctionCallArguments)
    }

    // Generate a fake temperature
    const temperature = Math.floor(Math.random() * (100 - 30 + 1) + 30)
    // Generate random weather condition
    const weather = ['sunny', 'cloudy', 'rainy', 'snowy'][
      Math.floor(Math.random() * 4)
    ]

    const functionResponse: ChatRequest = {
      messages: [
        ...chatMessages,
        {
          id: nanoid(),
          name: 'get_current_weather',
          role: 'function' as const,
          content: JSON.stringify({
            temperature,
            weather,
            info: 'This data is randomly generated and came from a fake weather API!'
          })
        }
      ]
    }
    return functionResponse
  }
}
```

Then just pass the handler to the hook:

```tsx
const { messages, input, handleInputChange, handleSubmit } = useChat({
  experimental_onFunctionCall: functionCallHandler
})
```

Now, when the model calls the `get_current_weather` function, the OpenAI API will return a specially formatted message with the arguments and the name of the function to call.
Your handler will then be invoked on the client to handle the function call and manipulate the chat accordingly.

### Rendering messages

The `Message` type has been updated with an optional `function_call` value that can either be an `CreateChatCompletionRequestFunction` object or a string.

A `CreateChatCompletionRequestFunction` looks like this:

```typescript
{
    /**
     * The name of the function to call.
     */
    name?: string;
    /**
     * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
     */
    arguments?: string;
}
```

So you can interact with the function call:

```tsx
{
  message.role === 'function' && message.function_call && (
    <div>
      <p>Function name: {message.function_call.name}</p>
      <p>Function arguments: {message.function_call.arguments}</p>
    </div>
  )
}
```

However, while a function is streaming in, it will not be valid JSON that can be parsed to a `CreateChatCompletionRequestFunction` object, so the `function_call` value will be a string instead.
If you want to stream in the functions raw data, you need to handle this special case:

```tsx
if (m.function_call) {
  const functionCallString =
    typeof m.function_call === 'string'
      ? m.function_call
      : JSON.stringify(m.function_call)

  return (
    <>
      {functionCallString.split('\\n').map((line, index) => (
        <p key={index}>{line}</p>
      ))}
    </>
  )
} else {
  return m.content
}
```

## On the server

To use functions with the OpenAI API, you need to pass the `functions` and `function_call` parameters to the API.
Here is an example Next.js Route Handler:

```
import { ChatCompletionFunctions } from 'openai-edge/types/api'

const functions: ChatCompletionFunctions[] = [
  {
    name: 'get_current_weather',
    description: 'Get the current weather',
    parameters: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'The city and state, e.g. San Francisco, CA'
        },
        format: {
          type: 'string',
          enum: ['celsius', 'fahrenheit'],
          description:
            'The temperature unit to use. Infer this from the users location.'
        }
      },
      required: ['location', 'format']
    }
  },
]

export async function POST(req: Request) {
  const { messages, function_call } = await req.json()

  const response = await openai.createChatCompletion({
    model: 'gpt-3.5-turbo-0613',
    stream: true,
    messages,
    functions,
    function_call
  })

  const stream = OpenAIStream(response)
  return new StreamingTextResponse(stream)
}
```
