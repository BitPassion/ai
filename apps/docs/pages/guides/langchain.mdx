---
title: LangChain
---

# LangChain

[LangChain](https://js.langchain.com/docs/) is a prompt engineering framework for developing applications powered by language models. It provides immensely useful tools and abstractions for working with AI models and agents.
However, LangChain does not provide a way to easily build UIs or a standard way to stream data to the client.

## Example

Here is an example implementation of a chat application that uses both AI SDK and LangChain's [OpenAIChat](https://js.langchain.com/docs/api/llms_openai/classes/OpenAIChat) together with [Next.js](https://nextjs.org/docs) App Router. It uses AI SDK's [`LangChainStream`](../api-reference#langchainstream) to stream text to the client (from the edge) and then AI SDK's `useChat` to handle the chat UI.

```tsx filename="app/api/chat/route.ts" {1,10,27}
import { StreamingTextResponse, LangChainStream } from 'ai-connector'
import { ChatOpenAI } from 'langchain/chat_models/openai'
import { AIChatMessage, HumanChatMessage } from 'langchain/schema'
import { CallbackManager } from 'langchain/callbacks'

export const runtime = 'edge'

export async function POST(req: Request) {
  const { messages } = await req.json()
  const { stream, handlers } = LangChainStream()

  const llm = new ChatOpenAI({
    streaming: true,
    callbackManager: CallbackManager.fromHandlers(handlers)
  })

  llm
    .call(
      messages.map(m =>
        m.role == 'user'
          ? new HumanChatMessage(m.content)
          : new AIChatMessage(m.content)
      )
    )
    .catch(console.error)

  return new StreamingTextResponse(stream)
}
```

The `handlers` object returned by `LangChainStream` is used to handle certain
[callbacks](https://js.langchain.com/docs/api/callbacks/) provided by LangChain on your behalf.
Under the hood it is a wrapper over LangChain's [`handleLLMEnd`](https://js.langchain.com/docs/api/callbacks/classes/BaseCallbackHandler#handlellmend), [`handleLLMNewToken`](https://js.langchain.com/docs/api/callbacks/classes/BaseCallbackHandler#handlellmnewtoken), and [`handleLLMError`](https://js.langchain.com/docs/api/callbacks/classes/BaseCallbackHandler#handlellmerror).
We wrap over these methods to provide which then write to the `stream` which can then be passed directly to [`StreamingTextResponse`](../api-reference#streamingtextresponse).

The result is that AI Util's [`useChat`](../api-reference#usechat) and [`useCompletion`](../api-reference#usecompletion) work flawlessly:

```tsx filename="app/page.tsx"
'use client'

import { useChat } from 'ai-connector'

export default function Chat() {
  const { messages, input, isLoading, handleInputChange, handleSubmit } =
    useChat()

  return (
    <div className="mx-auto w-full max-w-md py-24 flex flex-col stretch">
      {messages.length > 0
        ? messages.map(m => (
            <div key={m.id}>
              {m.role === 'user' ? 'User: ' : 'AI: '}
              {m.content}
            </div>
          ))
        : null}

      <form onSubmit={handleSubmit}>
        <input
          className="fixed w-full max-w-md bottom-0 border border-gray-300 rounded mb-8 shadow-xl p-2"
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />

        <button disabled={isLoading} type="submit">
          Send
        </button>
      </form>
    </div>
  )
}
```
