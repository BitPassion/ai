---
title: API Reference
---

# API Reference

## `useChat(options: UseChatOptions): ChatHelpers` [#usechat]

An SWR-powered React hook for streaming chat messages and handling chat and prompt input state.

```tsx filename="app/chat.tsx"
'use client'

import { useChat } from 'ai-connector'

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat()

  return (
    <div className="mx-auto w-full max-w-md py-24 flex flex-col stretch">
      {messages.length > 0
        ? messages.map(m => (
            <div key={m.id}>
              {m.role === 'user' ? 'User: ' : 'AI: '}
              {m.content}
            </div>
          ))
        : null}

      <form onSubmit={handleSubmit}>
        <input
          className="fixed w-full max-w-md bottom-0 border border-gray-300 rounded mb-8 shadow-xl p-2"
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />
      </form>
    </div>
  )
}
```

### `UseChatOptions`

- **`api?: string = '/api/chat'`** - The API endpoint that accepts a `{ messages: Message[] }` object and returns a stream of tokens of the AI chat response. Defaults to `/api/chat`.
- **`id?: string`** - An unique identifier for the chat. If not provided, a random one will be generated. When provided, the `useChat` hook with the same `id` will have shared states across components thanks to SWR.
- **`initialInput?: string = ''`** - An optional string of initial prompt input
- **`initialMessages?: Messages[] = []`** - An optional array of initial chat messages
- **`onResponse?: (res: Response) => void`** - An optional callback that will be called with the response from the API endpoint. Useful for throwing customized errors or logging
- **`onFinish?: (message: Message) => void`** - An optional callback that will be called when the chat stream ends
- **`headers?: Record<string, string> | Headers`** - An optional object of headers to be passed to the API endpoint
- **`body?: any`** - An optional, extra body object to be passed to the API endpoint

## `useCompletion(options: UseCompletionOptions): CompletionHelpers` [#usecompletion]

An SWR-powered React hook for streaming text completion and handling prompt input state.

```tsx filename="app/completion.tsx"
'use client'

import { useCompletion } from 'ai-connector'

export default function Completion() {
  const {
    completion,
    input,
    stop,
    isLoading,
    handleInputChange,
    handleSubmit
  } = useCompletion({
    api: '/api/some-custom-endpoint'
  })

  return (
    <div className="mx-auto w-full max-w-md py-24 flex flex-col stretch">
      <form onSubmit={handleSubmit}>
        <input
          className="fixed w-full max-w-md bottom-0 border border-gray-300 rounded mb-8 shadow-xl p-2"
          value={input}
          placeholder="Say something..."
          onChange={handleInputChange}
        />
        <p>Completion result: {completion}</p>
        <button type="button" onClick={stop}>
          Stop
        </button>
        <button disabled={isLoading} type="submit">
          Send
        </button>
      </form>
    </div>
  )
}
```

### `UseCompletionOptions`

- **`api?: string = '/api/completion'`** - The API endpoint that accepts a `{ prompt: string }` object and returns a stream of tokens of the AI completion response. Defaults to `/api/completion`
- **`id?: string`** - An unique identifier for the completion. If not provided, a random one will be generated. When provided, the `useCompletion` hook with the same `id` will have shared states across components thanks to SWR
- **`initialInput?: string = ''`** - An optional string of initial prompt input
- **`initialCompletion?: string = ''`** - An optional string of initial completion result
- **`onResponse?: (res: Response) => void`** - An optional callback that will be called with the response from the API endpoint. Useful for throwing customized errors or logging
- **`onFinish?: (prompt: string, completion: string) => void`** - An optional callback that will be called when the completion stream ends
- **`headers?: Record<string, string> | Headers`** - An optional object of headers to be passed to the API endpoint
- **`body?: any`** - An optional, extra body object to be passed to the API endpoint

## `OpenAIStream(res: Response, cb: AIStreamCallbacks): ReadableStream` [#openaistream]

A transform that will extract the text from all chat and completion OpenAI models as returned as a `ReadableStream`.

```tsx filename="app/api/chat/route.ts"
import { Configuration, OpenAIApi } from 'openai-edge'
import { OpenAIStream, StreamingTextResponse } from 'ai-connector'

const config = new Configuration({
  apiKey: process.env.OPENAI_API_KEY
})
const openai = new OpenAIApi(config)

export const runtime = 'edge'

export async function POST() {
  const response = await openai.createChatCompletion({
    model: 'gpt-4',
    stream: true,
    messages: [{ role: 'user', content: 'What is love?' }]
  })
  const stream = OpenAIStream(response, {
    async onStart() {
      console.log('streamin yo')
    },
    async onToken(token) {
      console.log('token: ' + token)
    },
    async onCompletion(content) {
      console.log('full text: ' + content)
      // await prisma.messages.create({ content }) or something
    }
  })
  return new StreamingTextResponse(stream)
}
```

## `HuggingFaceStream(iter: AsyncGenerator<any>, cb?: AIStreamCallbacks): ReadableStream` [#huggingfacestream]

A transform that will extract the text from _most_ chat and completion HuggingFace models and return them as a `ReadableStream`.
It expects the iterable `AsyncGenerator` from HuggingFace Inference SDK's `hf.textGenerationStream` method.

```tsx filename="app/api/chat/route.ts"
import { HfInference } from '@huggingface/inference'
import { HuggingFaceStream, StreamingTextResponse } from 'ai-connector'

export const runtime = 'edge'

const Hf = new HfInference(process.env.HUGGINGFACE_API_KEY)

export async function POST() {
  const response = await Hf.textGenerationStream({
    model: 'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',
    inputs: `<|prompter|>What's the Earth total population?<|endoftext|><|assistant|>`,
    parameters: {
      max_new_tokens: 200,
      // @ts-ignore
      typical_p: 0.2, // you'll need this for OpenAssistant
      repetition_penalty: 1,
      truncate: 1000,
      return_full_text: false
    }
  })
  const stream = HuggingFaceStream(response)
  return new StreamingTextResponse(stream)
}
```

### `LangChainStream(cb?: AIStreamCallbacks): { stream: ReadableStream; handlers: LangChainHandlers }` [#langchainstream]

Returns a `stream` and bag of [LangChain](js.langchain.com/docs) `BaseCallbackHandlerMethodsClass` that automatically implement streaming in such a way that you can use [`useChat`](#usechat) and [`useCompletion`](#usecompletion).

#### Example

Here is a reference implementation of a chat endpoint that uses both the Vercel AI SDK and LangChain together with Next.js App Router

```tsx filename="app/api/chat/route.ts"
import { StreamingTextResponse, LangChainStream } from 'ai-connector'
import { ChatOpenAI } from 'langchain/chat_models/openai'
import { AIChatMessage, HumanChatMessage } from 'langchain/schema'
import { CallbackManager } from 'langchain/callbacks'

export const runtime = 'edge'

export async function POST(req: Request) {
  const { messages } = await req.json()
  const { stream, handlers } = LangChainStream()

  const llm = new ChatOpenAI({
    streaming: true,
    callbackManager: CallbackManager.fromHandlers(handlers)
  })

  llm
    .call(
      messages.map(m =>
        m.role == 'user'
          ? new HumanChatMessage(m.content)
          : new AIChatMessage(m.content)
      )
    )
    .catch(console.error)

  return new StreamingTextResponse(stream)
}
```

## `StreamingTextResponse(res: ReadableStream, init?: ResponseInit)` [#streamingtextresponse]

This is a tiny wrapper around `Response` class that makes returning `ReadableStreams` of text a one liner. Status is automatically set to `200`, with `'Content-Type': 'text/plain; charset=utf-8'` set as `headers`.

```tsx
// app/api/generate/route.ts
import { OpenAIStream, StreamingTextResponse } from 'ai-connector'

export const runtime = 'edge'

export async function POST() {
  const response = await openai.createChatCompletion({
    model: 'gpt-4',
    stream: true,
    messages: [{ role: 'user', content: 'What is love?' }]
  })
  const stream = OpenAIStream(response)
  return new StreamingTextResponse(stream, {
    'X-RATE-LIMIT': 'lol'
  }) // => new Response(stream, { status: 200, headers: { 'Content-Type': 'text/plain; charset=utf-8', 'X-RATE-LIMIT': 'lol' }})
}
```
